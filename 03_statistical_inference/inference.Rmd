---
title: "Inference and Modeling"
author: "Raúl López Domínguez"
date: "`r Sys.Date()`"
output: html_notebook
---

# Inference

Statistical inference is the part of statistics that helps distinguish patterns arising from signal from those arising from chance. Statistical inference is a broad topic and here we go over the very basics using polls as a motivating example. To describe the concepts, we complement the mathematical formulas with Monte Carlo simulations and R code.

## Sampling Model Parameters and Estimates

We want to predict the proportion of the blue beads in the urn, the parameter p. The proportion of red beads in the urn is 1-p and the spread is 2p-1.

In a sampling model, the collection of elements in the urn is called the population. The proportion of blue beads in the population p is called parameter. The number of draws is the sample. The task of statistical inference is to estimate an unknown population parameter using observed data from a sample.

### Sample Average

Conducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter p. We start by defining the random variable X as X = 1 if we pick a blue bead and X = 0 if it is red. When we sample N beads, the average of the draws (X1, X2, Xn) is equivalent to the proportion of blue beads in our sample. We use the symbol ^X to represent this average. We can calculate X^ as a sum of draw divided by N.

```{r}
set.seed(123)
N <- 25
draw_res <- sample(c(0,1), N, replace = T)
sum_draw_res <- sum(draw_res)
X_avg <- sum_draw_res / N
X_avg
```

We know that the average of the 0s and 1s in the urn must be p, the proportion of blue beads. Here we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to estimate p.

### Parameters

Just like we use variables to define unknowns in systems of equations, in statistical inference we define parameters to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters p to represent this quantity. p is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is p, we are going to estimate this parameter.

### Properties of the estimates. EXpected value and standard error

To understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion ^X. Remember that ^X is the sum of independent draws so the rules we covered in the probability chapter apply.

As E[X] = N\*p; the expected value of the avg ^X is N\*p/N; so E[^X] = p

As SE[E] = sqrt(N\*SD; so SE[^X] = sqrt(p*(1-p)/N)

This result reveals the power of polls. The expected value of the sample proportion ^X is the parameter of interest p and we can make the standard error as small as we want by increasing N. The law of large numbers tells us that with a large enough poll, our estimate converges to p.

If we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?

One problem is that we do not know p, so we can’t compute the standard error. However, for illustrative purposes, let’s assume that p=0.51 and make a plot of the standard error versus the sample size N:

```{r}
library(ggplot2)

p <- 0.51
Ns <- seq(10,10000, 10)
se_values <- sapply(Ns, function(N){
  sqrt(p*(1-p)/N)
})

se_df <- data.frame(se = se_values, N = Ns)

ggplot(se_df, aes(x = N, y = se))+
  geom_line()
```

From the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table, we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and p=0.51, the standard error is:

```{r}
sqrt(p*(1-p))/sqrt(1000)
```

or 1.5 percentage points. So even with large polls, for close elections, ^X can lead us astray if we don’t realize it is a random variable

## Exercises

1) Suppose you poll a population in which a proportion p of voters are Democrats and p-1 are Republicans. Your sample size is N = 25. Consider the random variable S, which is the total number of Democrats in your sample. What is the expected value of this random variable S?

E[S] = Np

2) What is the standard error of S?

SE[S] = sqrt(Np(1-p))

3) Consider the random variable S/N, which is equivalent to the sample average that we have been denoting as ^X. The variable N represents the sample size and p is the proportion of Democrats in the population. What is the expected value of?

E[^X] = p

4) What is the standard error of the sample average, ^X?

SE[^X] = sqrt(p(1-p)/N)

5) Write a line of code that calculates the standard error se of a sample average when you poll 25 people in the population. Generate a sequence of 100 proportions of Democrats p that vary from 0 (no Democrats) to 1 (all Democrats). Plot se versus p for the 100 different proportions.

```{r}
# `N` represents the number of people polled
N <- 25

# Create a variable `p` that contains 100 proportions ranging from 0 to 1 using the `seq` function
p <- seq(0,1, length = 100)

# Create a variable `se` that contains the standard error of each sample average
se <- sqrt(p * (1 - p)/N)

# Plot `p` on the x-axis and `se` on the y-axis
plot(p,se)
```

6) Using the same code as in the previous exercise, create a for-loop that generates three plots of p versus se when the sample sizes equal N = 25, N = 100, and N = 1000.

```{r}
# The vector `p` contains 100 proportions of Democrats ranging from 0 to 1 using the `seq` function
p <- seq(0, 1, length = 100)

# The vector `sample_sizes` contains the three sample sizes
sample_sizes <- c(25, 100, 1000)

# Write a for-loop that calculates the standard error `se` for every value of `p` for each of the three samples sizes `N` in the vector `sample_sizes`. Plot the three graphs, using the `ylim` argument to standardize the y-axis across all three plots.
for(N in sample_sizes){
  se <- sqrt(p*(1-p)/N)
  plot(p, se, ylim = c(0,0.5/sqrt(25)))
}
```

7) Our estimate for the difference in proportions of Democrats and Republicans is d = ^X - (1 - ^X). Which derivation correctly uses the rules we learned about sums of random variables and scaled random variables to derive the expected value of?.

E[^X - (1 - ^X)] = E[2^X -1] = 2E[^X] - 1 = 2p -1 = p - (1 - p)

8) Which derivation correctly uses the rules we learned about sums of random variables and scaled random variables to derive the standard error of d?

SE[^X - (1 - ^X)] = SE[2^X -1] = 2SE[^X] = 2sqrt(p(1-p)/N)

9) Say the actual proportion of Democratic voters is p = 0.45. In this case, the Republican party is winning by a relatively large margin of d = -0.1, or a 10% margin of victory. What is the standard error of the spread 2^X - 1 in this case?

```{r}
# `N` represents the number of people polled
N <- 25

# `p` represents the proportion of Democratic voters
p <- 0.45

# Calculate the standard error of the spread. Print this value to the console.
#SE[^X - (1 - ^X)] = SE[2^X -1] = 2SE[^X] = 2sqrt(p(1-p)/N)

2*sqrt(p*(1-p)/N)
```

10) So far we have said that the difference between the proportion of Democratic voters and Republican voters is about 10% and that the standard error of this spread is about 0.2 when N=25. Select the statement that explains why this sample size is sufficient or not. This sample size is too small because the standard error is larger than the spread.

## The Central Limit Theorem in Practice

CTL tells us that the distribution function for a sum of draws is approximately normal, so the distribution of ^X is also normal. Suppose we want to know the probability that we are within 1% from p:

Pr(^X - p) <= 0.01

Pr(^X <= p + 0.01) - Pr(^X <= p - 0.01)

Subtract the expected value and divide by the standard error to get a standard normal random variable, call it Z, on the left.

Pr(Z <= (0.01/SE[^X])) - Pr(Z <= (-0.01/SE[^X]))

One problem we have is that since we don’t know p, we don’t know SE(^X). But it turns out that the CLT still works if we estimate the standard error by using ^X in place of p. We say that we plug-in the estimate. Our estimate of the standard error is therefore:

SE_hat[^X] = sqrt(^X\*(1-^X)/N)

```{r}
x_hat <- 0.48
se <- sqrt(x_hat*(1-x_hat)/25)
se
```

And now we can answer the question of the probability of being close to p. The answer is:

```{r}
pnorm(0.01/se) - pnorm(-0.01/se)
```

Therefore, there is a small chance that we will be close. A poll of only N=25 people is not really very useful, at least not for a close election.

Earlier we mentioned the margin of error. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is:

```{r}
1.96*se
```

Why do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from p, we get:

Pr(Z <= 1.96) - Pr(Z <= -1.96)

which is the 95%

```{r}
pnorm(1.96) - pnorm(-1.96)
```

### Monte Carlo Simulation

Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:

```{r}
B <- 10000
N <- 1000
x_hat <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})
```

The problem is, of course, we don’t know p. One thing we therefore do to corroborate theoretical results is to pick one or several values of p and run the simulations. Let’s set p=0.45. We can then simulate a poll

```{r}
p <- 0.45
N <- 1000

x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat <- mean(x)

```

In this particular sample, our estimate is x_hat. We can use that code to do a Monte Carlo simulation:

```{r}
B <- 10000
x_hat <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})

mean(x_hat)
sd(x_hat)
```

Of course, in real life we would never be able to run such an experiment because we don’t know p. But we could run it for various values of p and N and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing p and N


### Spread

The competition is to predict the spread, not the proportion p. However, because we are assuming there are only two parties, we know that the spread is p−(1−p)=2p−1. As a result, everything we have done can easily be adapted to an estimate of 2p−1. Once we have our estimate ^X and ^SE(¯X), we estimate the spread with 2^X−1 and, since we are multiplying by 2, the standard error is 2^SE(¯X). Note that subtracting 1 does not add any variability so it does not affect the standard error.

## Exercises

1) Write an urn model function that takes the proportion of Democrats p and the sample size N as arguments and returns the sample average if Democrats are 1s and Republicans are 0s. Call the function take_sample

```{r}
# Write a function called `take_sample` that takes `p` and `N` as arguements and returns the average value of a randomly sampled population.
take_sample <- function(p, N){
  draws <- sample(c(1,0), size = N, replace = T, prob = c(p,1-p))
  mean(draws)
}


# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# Call the `take_sample` function to determine the sample average of `N` randomly selected people from a population containing a proportion of Democrats equal to `p`. Print this value to the console.
take_sample(p,N)
```

2) Now assume p <- 0.45 and that your sample size is N=100. Take a sample 10,000 times and save the vector of mean(X) - p into an object called errors. Hint: use the function you wrote for exercise 1 to write this in one line of code.

```{r}
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Create an objected called `errors` that replicates subtracting the result of the `take_sample` function from `p` for `B` replications
errors <- replicate(B, p - take_sample(p, N))

# Calculate the mean of the errors. Print this value to the console.
mean(errors)
```

3) The vector errors contains, for each simulated sample, the difference between the actual p and our estimate ¯X. We refer to this difference as the error. Compute the average and make a histogram of the errors generated in the Monte Carlo simulation and select which of the following best describes their distributions:

```{r}
mean(errors)
hist(errors)
```

The errors are symmetrically distributed around 0.

4) The error ¯X−p is a random variable. In practice, the error is not observed because we do not know p. Here we observe it because we constructed the simulation. What is the average size of the error if we define the size by taking the absolute value ∣¯X−p∣?

```{r}
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
errors <- replicate(B, p - take_sample(p, N))

# Calculate the mean of the absolute value of each simulated error. Print this value to the console.
mean(abs(errors))
```

5) The standard error is related to the typical size of the error we make when predicting. We say size because we just saw that the errors are centered around 0, so thus the average error value is 0. For mathematical reasons related to the Central Limit Theorem, we actually use the standard deviation of errors rather than the average of the absolute values to quantify the typical size. What is this standard deviation of the errors?

```{r}
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
errors <- replicate(B, p - take_sample(p, N))

# Calculate the standard deviation of `errors`
sqrt(mean(errors^2))
```

6) The theory we just learned tells us what this standard deviation is going to be because it is the standard error of ¯X. What does theory tell us is the standard error of ¯X for a sample size of 100?

```{r}
# Define `p` as the expected value equal to 0.45
p <- 0.45

# Define `N` as the sample size
N <- 100

# Calculate the standard error
se <- sqrt(p*(1-p)/N)
se
```

7) In practice, we don't know p, so we construct an estimate of the theoretical prediction based by plugging in ^X
for p. Calculate the standard error of the estimate: SE_avg[^X]

```{r}
# Define `p` as a proportion of Democratic voters to simulate
p <- 0.45

# Define `N` as the sample size
N <- 100

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `X` as a random sample of `N` voters with a probability of picking a Democrat ('1') equal to `p`
X <- sample(c(1,0), replace = T, size = N, prob = c(p,1-p))

# Define `X_bar` as the average sampled proportion
X_bar <- mean(X)

# Calculate the standard error of the estimate. Print the result to the console.
se <- sqrt(X_bar*(1-X_bar)/N)
se
```

8) The standard error estimates obtained from the Monte Carlo simulation, the theoretical prediction, and the estimate of the theoretical prediction are all very close, which tells us that the theory is working. This gives us a practical approach to knowing the typical error we will make if we predict p with ^X. The theoretical result gives us an idea of how large a sample size is required to obtain the precision we need. Earlier we learned that the largest standard errors occur for p = 0.5. Create a plot of the largest standard error for
ranging from 100 to 5,000. Based on this plot, how large does the sample size have to be to have a standard error of about 1%?

```{r}
N <- seq(100, 5000, len = 100)
p <- 0.5
se <- sqrt(p*(1-p)/N)

ggplot(data.frame(se, N), aes(x = se, y = N))+geom_line()
```

9) For N=100, the central limit theorem tells us that the distribution of ^X is... Approximately normal with expected value p and standard error sqrt((p\*(1-p))/N).

10) We calculated a vector errors that contained, for each simulated sample, the difference between the actual value p and our estimate ^X. The errors ^X - p are approximately normal with expected value $0$ and standard error $\sqrt{p(1-p)/N}$.

11) To corroborate your answer to exercise 9, make a qq-plot of the `errors` you generated in exercise 2 to see if they follow a normal distribution.

```{r}
qqnorm(errors)
qqline(errors)
```

12) If $p=0.45$ and $N=100$ as in exercise 2, use the CLT to estimate the probability that $\bar{X}>0.5$. You can assume you know $p=0.45$ for this calculation.

```{r}
p <- 0.45
N <- 100
1 - pnorm(0.5, p, sqrt(p*(1-p)/N))
```

13) Assume you are in a practical situation and you don't know $p$. Take a sample of size $N=100$ and obtain a sample average of $\bar{X} = 0.51$. What is the CLT approximation for the probability that your error is equal or larger than 0.01?

```{r}
N <-100
X_hat <- 0.51
se_hat <- sqrt(X_hat*(1-X_hat)/N)
1 - pnorm(.01, 0, se_hat) + pnorm(-0.01, 0, se_hat)
```

## Confidence Intervals

Confidence intervals are a very useful concept widely employed by data analysts. A version of these that are commonly seen come from the ggplot geometry geom_smooth.

In our earlier competition, you were asked to give an interval. If the interval you submitted includes the p, you get half the money you spent on your “poll” back and pass to the next stage of the competition. One way to pass to the second round is to report a very large interval. For example, the interval [0,1] is guaranteed to include p. However, with an interval this big, we have no chance of winning the competition. Similarly, if you are an election forecaster and predict the spread will be between -100% and 100%, you will be ridiculed for stating the obvious. Even a smaller interval, such as saying the spread will be between -10 and 10%, will not be considered serious.

On the other hand, the smaller the interval we report, the smaller our chances are of winning the prize. Likewise, a bold pollster that reports very small intervals and misses the mark most of the time will not be considered a good pollster. We want to be somewhere in between.

We can use the statistical theory we have learned to compute the probability of any given interval including p. If we are asked to create an interval with, say, a 95% chance of including p, we can do that as well. These are called 95% confidence intervals.

We want to know the probability that the interval [¯X−2^SE(¯X),¯X+2^SE(¯X)] contains the true proportion p. First, consider that the start and end of these intervals are random variables: every time we take a sample, they change.

```{r}
p <- 0.45
N <- 1000

set.seed(123)
x <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat <- mean(x)
se_hat <- sqrt(x_hat * (1 - x_hat) / N)
c(x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)

set.seed(189)
x <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat <- mean(x)
se_hat <- sqrt(x_hat * (1 - x_hat) / N)
c(x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)
```

To determine the probability that the interval includes p, we need to compute this:

Pr(^X - 1.96^SE(^X) <= p <= ^X + 1.96^SE(^X))
Pr(-1.96 <= Z <= 1.96)

```{r}
pnorm(1.96) - pnorm(-1.96)
```

If we want a larger probability, we need to multiply by whatever Z satisfies the following:

Pr(-z <= Z <= z) = 0.99

```{r}
p <- 0.995
z <- qnorm(p)
pnorm(z) - pnorm(-z)
```

We can use this approach for any probability, not just 0.95 and 0.99. In statistics textbooks, these are usually written for any probability as 1−α. We can then obtain the z for the equation above noting using 
z = qnorm(1 - alpha / 2) because 1−α/2−α/2=1−α.

So, for example, for α=0.05, 1−α/2=0.975 and we get the 1.96 we have been using:

```{r}
qnorm(0.975)


# expected p

p <- c(0.9, 0.95, 0.99)
a <- 1 - p
z <- qnorm(1-a/2)
pnorm(z) - pnorm(-z)
```

### Monte Carlo Simulation

We can run a Monte Carlo simulation to confirm that, in fact, a 95% confidence interval includes p 95% of the time

```{r}
library(dplyr)
p <- 0.45
N <- 1000
B <- 10000
inside <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  x_hat <- mean(x)
  se_hat <- sqrt(x_hat * (1 - x_hat) / N)
  between(p, x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)
})
mean(inside)
```


## Exercises

For these exercises, we will use actual polls from the 2016 election. You can load the data from the dslabs package.

Specifically, we will use all the national polls that ended within one week before the election.

1) Assume there are only two candidates and construct a 95% confidence interval for the election night proportion.

```{r}
library(tidyverse)
library(dslabs)
data(polls_us_election_2016)

# Generate an object `polls` that contains data filtered for polls that ended on or after October 31, 2016 in the United States
polls <- polls_us_election_2016 |> 
  filter(enddate >= "2016-10-31" & state == "U.S.")


# How many rows does `polls` contain? Print this value to the console.
nrow(polls)

# Assign the sample size of the first poll in `polls` to a variable called `N`. Print this value to the console.
N <- head(polls$samplesize,1)
N


# For the first poll in `polls`, assign the estimated percentage of Clinton voters to a variable called `X_hat`. Print this value to the console.
X_hat <- head(polls$rawpoll_clinton,1)/100
X_hat

# Calculate the standard error of `X_hat` and save it to a variable called `se_hat`. Print this value to the console.
se_hat <- sqrt(X_hat * (1 - X_hat) / N)
se_hat

# Use `qnorm` to calculate the 95% confidence interval for the proportion of Clinton voters. Save the lower and then the upper confidence interval to a variable called `ci`.
a <- 0.05
z <- qnorm(1-a/2)
ci <- c(X_hat - z * se_hat, X_hat + z * se_hat)
ci
```

2) Now use dplyr to add a confidence interval as two columns, call them lower and upper, to the object poll. Then use select to show the pollster, enddate, x_hat,lower, upper variables. Hint: define temporary columns x_hat and se_hat.

```{r}
# Create a new object called `pollster_results` that contains columns for pollster name, end date, X_hat, se_hat, lower confidence interval, and upper confidence interval for each poll.
polls <- mutate(polls, X_hat = polls$rawpoll_clinton/100, se_hat = sqrt(X_hat*(1-X_hat)/polls$samplesize), lower = X_hat - qnorm(0.975)*se_hat, upper = X_hat + qnorm(0.975)*se_hat)
pollster_results <- select(polls, pollster, enddate, X_hat, se_hat, lower, upper)
pollster_results
```

3) The final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column, call it hit, to the previous table stating if the confidence interval included the true proportion p=0.482 or not.

```{r}
# Add a logical variable called `hit` that indicates whether the actual value exists within the confidence interval of each poll. Summarize the average `hit` result to determine the proportion of polls with confidence intervals include the actual value. Save the result as an object called `avg_hit`.
avg_hit <- pollster_results %>% mutate(hit=(lower<0.482 & upper>0.482))
avg_hit
```

4) For the table you just created, what proportion of confidence intervals included p?

```{r}
avg_hit %>% summarise(mean(hit))
```


5) If these confidence intervals are constructed correctly, and the theory holds up, what proportion of confidence intervals should include p? - 0.95

6) A much smaller proportion of the polls than expected produce confidence intervals containing p. If you look closely at the table, you will see that most polls that fail to include p are underestimating. The reason for this is undecided voters, individuals polled that do not yet know who they will vote for or do not want to say. Because, historically, undecideds divide evenly between the two main candidates on election day, it is more informative to estimate the spread or the difference between the proportion of two candidates d, which in this election was 0.482−0.461=0.021.

```{r}
# contrast that p is understimate

ggplot(avg_hit)+
  geom_point(aes(y = pollster, x = X_hat, color = hit))+
  geom_vline(xintercept = 0.482)
```

```{r}
# Add a statement to this line of code that will add a new column named `d_hat` to `polls`. The new column should contain the difference in the proportion of voters.
polls <- polls_us_election_2016 %>% filter(enddate >= "2016-10-31" & state == "U.S.") %>% 
  mutate(d_hat = rawpoll_clinton/100 - rawpoll_trump/100)

```

Assume that there are only two parties and that d=2p−1, redefine polls as below and re-do exercise 1, but for the difference.

```{r}
N <- polls$samplesize[1]
d_hat <- polls$d_hat[1]
X_hat <- (d_hat+1)/2
se_hat <- 2*sqrt(X_hat*(1-X_hat)/N)
d_hat + c(-1,1)*pnorm(0.975)*se_hat
```

7) Now repeat exercise 3, but for the difference.
```{r}
polls %>% mutate(X_hat = (d_hat+1)/2, se_hat = 2*sqrt(X_hat*(1-X_hat)/samplesize),
                 lower = d_hat - pnorm(0.975)*se_hat, upper = d_hat + pnorm(0.975)*se_hat, 
                 hit = lower<=0.021 & upper>=0.021) %>%
  select(pollster, enddate, d_hat, lower, upper, hit) 
```

8) Now repeat exercise 4

```{r}
polls %>% mutate(X_hat = (d_hat+1)/2, se_hat = 2*sqrt(X_hat*(1-X_hat)/samplesize),
                 lower = d_hat - pnorm(0.975)*se_hat, upper = d_hat + pnorm(0.975)*se_hat, 
                 hit = lower<=0.021 & upper>=0.021) %>%
  select(pollster, enddate, d_hat, lower, upper, hit) %>% 
  summarize(mean(hit))
```

9) Although the proportion of confidence intervals goes up substantially, it is still lower than 0.95. In the next chapter, we learn the reason for this. To motivate this, make a plot of the error, the difference between each poll’s estimate and the actual d=0.021. Stratify by pollster.

```{r}
polls %>% mutate(error = d_hat - 0.021) %>%
  ggplot(aes(pollster, error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

10) Re-do the plot but for pollsters that took more than 5 polls

```{r}
polls %>% mutate(error = d_hat - 0.021) %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>%
  ggplot(aes(pollster, error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


## Power

Pollsters are not successful at providing correct confidence intervals, but rather at predicting who will win. When we took a 25 bead sample size, the confidence interval for the spread:

```{r}
N <- 25
x_hat <- 0.48
(2 * x_hat - 1) + c(-1.96, 1.96) * 2 * sqrt(x_hat * (1 - x_hat) / N)
```

includes 0. If this were a poll and we were forced to make a declaration, we would have to say it was a “toss-up”.

A problem with our poll results is that given the sample size and the value of p, we would have to sacrifice on the probability of an incorrect call to create an interval that does not include 0. This does not mean that the election is close. It only means that we have a small sample size. In statistical textbooks this is called lack of power. In the context of polls, power is the probability of detecting spreads different from 0.

By increasing our sample size, we lower our standard error and therefore have a much better chance of detecting the direction of the spread.

## p-values

p-values are ubiquitous in the scientific literature. They are related to confidence intervals so we introduce the concept here. Let’s consider the blue and red beads. Suppose that rather than wanting an estimate of the spread or the proportion of blue, I am interested only in the question: are there more blue beads or red beads? I want to know if the spread or, equivalently, if p>0.5.

Say we take a random sample of N=100 and we observe 52 blue beads, which gives us ¯X=0.52. This seems to be pointing to the existence of more blue than red beads since 0.52 is larger than 0.5. However, as data scientists we need to be skeptical. We know there is chance involved in this process and we could get a 52 even when the actual spread is 0. We call the assumption that the spread is p=0.5 a null hypothesis. The null hypothesis is the skeptic’s hypothesis. We have observed a random variable ¯X=0.52 and the p-value is the answer to the question: how likely is it to see a value this large, when the null hypothesis is true? So we write:

Pr(|^X - 0.5| > 0.02)

Assuming the p = 0.5. Under the null hypothesis we know that:

sqr(N) \* (^x - 0.5)/sqrt(0.5*(1-0.5))

```{r}
N <- 100
z <- sqrt(N)*0.02/0.5
1 - (pnorm(z) - pnorm(-z))
```

In this case, there is actually a large chance of seeing 52 or larger under the null hypothesis. Keep in mind that there is a close connection between p-values and confidence intervals. If a 95% confidence interval of the spread does not include 0, we know that the p-value must be smaller than 0.05.



# Statistical Model

The day before the 2008 presidential election, Nate Silver’s FiveThirtyEight stated that “Barack Obama appears poised for a decisive electoral victory”. They went further and predicted that Obama would win the election with 349 electoral votes to 189, and the popular vote by a margin of 6.1%. FiveThirtyEight also attached a probabilistic statement to their prediction claiming that Obama had a 91% chance of winning the election. The predictions were quite accurate since, in the final results, Obama won the electoral college 365 to 173 and the popular vote by a 7.2% difference. Their performance in the 2008 election brought FiveThirtyEight to the attention of political pundits and TV personalities. Four years later, the week before the 2012 presidential election, FiveThirtyEight’s Nate Silver was giving Obama a 90% chance of winning despite many of the experts thinking the final results would be closer.

In this chapter we will demonstrate how poll aggregators, such as FiveThirtyEight, collected and combined data reported by different experts to produce improved predictions. We will introduce ideas behind the statistical models, also known as probability models, that were used by poll aggregators to improve election forecasts beyond the power of individual polls.

As we described earlier, a few weeks before the 2012 election Nate Silver was giving Obama a 90% chance of winning. How was Mr. Silver so confident? We will use a Monte Carlo simulation to illustrate the insight Mr. Silver had and others missed. To do this, we generate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls and construct and report 95% confidence intervals for each of the 12 polls. We save the results from this simulation in a data frame and add a poll ID column.

```{r}
library(tidyverse)
library(dslabs)
d <- 0.039
Ns <- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)
p <- (d + 1) / 2

polls <- map_df(Ns, function(N) {
  x <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
  x_hat <- mean(x)
  se_hat <- sqrt(x_hat * (1 - x_hat) / N)
  list(estimate = 2 * x_hat - 1, 
    low = 2*(x_hat - 1.96*se_hat) - 1, 
    high = 2*(x_hat + 1.96*se_hat) - 1,
    sample_size = N)
}) |> mutate(poll = as.factor(seq_along(Ns)))

ggplot(polls, aes(x = estimate, y = poll)) +
  geom_point(color = "deepskyblue")+
  geom_segment(aes(x = low, xend = high, y = poll, yend = poll), color = "deepskyblue")
```

Not surprisingly, all 12 polls report confidence intervals that include the election night result (dashed line). However, all 12 polls also include 0 (solid black line) as well. Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up. Below we describe a key insight they are missing.

Poll aggregators, such as Nate Silver, realized that by combining the results of different polls you could greatly improve precision. By doing this, we are effectively conducting a poll with a huge sample size. We can therefore report a smaller 95% confidence interval and a more precise prediction.

Although as aggregators we do not have access to the raw poll data, we can use mathematics to reconstruct what we would have obtained had we made one large poll with:

```{r}
sum(polls$sample_size)
```

participants. Basically, we construct an estimate of the spread, let’s call it d, with a weighted average in the following way:

```{r}
d_hat <- polls |> 
  summarize(avg = sum(estimate*sample_size) / sum(sample_size)) |> 
  pull(avg)
```

Once we have an estimate of d, we can construct an estimate for the proportion voting for Obama, which we can then use to estimate the standard error. Once we do this, we see that our margin of error is 0.018. Thus, we can predict that the spread will be 3.1 plus or minus 1.8, which not only includes the actual result we eventually observed on election night, but is quite far from including 0. Once we combine the 12 polls, we become quite certain that Obama will win the popular vote.

### Poll Data

```{r}
data(polls_us_election_2016)
```

The table includes results for national polls, as well as state polls, taken during the year prior to the election. For this first example, we will filter the data to include national polls conducted during the week before the election. We also remove polls that FiveThirtyEight has determined not to be reliable and graded with a “B” or less. Some polls have not been graded and we include those and add a spread estimate

```{r}
polls <- polls_us_election_2016 |> 
  filter(state == "U.S." & enddate >= "2016-10-31" &
           (grade %in% c("A+","A","A-","B+") | is.na(grade))) |>
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
```

For this example, we will assume that there are only two parties and call p the proportion voting for Clinton and 1−p the proportion voting for Trump. We are interested in the spread 2p−1. Let’s call the spread d (for difference)

We have 49 estimates of the spread. The theory we learned tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread d and the standard error is 2\*sqrt(p(1−p)/N). Assuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread (d_hat) and the standar error (moe)

```{r}
d_hat <- polls |>
  summarize(d_hat = sum(spread*samplesize) / sum(samplesize),
            p_hat = (d_hat+1)/2,
            moe = 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(samplesize)))

d_hat
```

So we report a spread of 1.43% with a margin of error of 0.66%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened?

```{r}
polls |>
  ggplot(aes(spread)) +
  geom_histogram(color="black", binwidth = .01)
```

The data does not appear to be normally distributed and the standard error appears to be larger than 0.007. The theory is not quite working here.

## Pollster bias

Notice that various pollsters are involved and some are taking several polls a week:

```{r}
polls |> group_by(pollster) |> summarize(n())

polls |> ggplot(aes(x = spread, y = pollster))+geom_point()+theme_minimal()
```

This plot reveals an unexpected result. First, consider that the standard error predicted by theory for each poll:


```{r}
polls |> group_by(pollster) |> 
  filter(n() >= 6) |>
  summarize(d_hat = sum(spread*samplesize) / sum(samplesize),
            p_hat = (d_hat+1)/2,
            moe = 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(samplesize)),
            se = 2 * sqrt(p_hat * (1-p_hat) / median(samplesize))) %>%
  select(pollster, se)
```

is between 0.018 and 0.033, which agrees with the within poll variation we see. However, there appears to be differences across the polls. Note, for example, how the USC Dornsife/LA Times pollster is predicting a 4% win for Trump, while Ipsos is predicting a win larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values. All the polls should have the same expected value. FiveThirtyEight refers to these differences as “house effects”. We also call them pollster bias.

## Data-driven models

For each pollster, let’s collect their last reported result before the election:

```{r}
one_poll_per_pollster <- polls |> group_by(pollster) |> 
  filter(enddate == max(enddate)) |>
  ungroup()

ggplot(one_poll_per_pollster, aes(x = spread))+geom_histogram()
```

In the previous section, we saw that using the urn model theory to combine these results might not be appropriate due to the pollster effect. Instead, we will model this spread data directly.

The new model can also be thought of as an urn model, although the connection is not as direct. Rather than 0s (Republicans) and 1s (Democrats), our urn now contains poll results from all possible pollsters. We assume that the expected value of our urn is the actual spread d=2p−1.

Because instead of 0s and 1s, our urn contains continuous numbers between -1 and 1, the standard deviation of the urn is no longer √p(1−p). Rather than voter sampling variability, the standard error now includes the pollster-to-pollster variability. Our new urn also includes the sampling variability from the polling. Regardless, this standard deviation is now an unknown parameter. In statistics textbooks, the Greek symbol σ is used to represent this parameter. In summary, we have two unknown parameters: the expected value d and the standard deviation σ.

Our task is to estimate d. Because we model the observed values X1,…XN as a random sample from the urn, the CLT might still work in this situation because it is an average of independent random variables. For a large enough sample size N, the probability distribution of the sample average ¯X is approximately normal with expected value μ and standard error σ/√N. If we are willing to consider N=15 large enough, we can use this to construct confidence intervals. A problem is that we don’t know σ. But theory tells us that we can estimate the urn model σ with the sample standard deviation defined as

s = sqrt(sum(Xi - ^X)^2 / (N-1))

```{r}
sd(one_poll_per_pollster$spread)
```

We are now ready to form a new confidence interval based on our new data-driven model:

```{r}
results <- one_poll_per_pollster |> 
  summarize(avg = mean(spread), 
            se = sd(spread) / sqrt(length(spread))) |> 
  mutate(start = avg - 1.96 * se, 
         end = avg + 1.96 * se) 
round(results * 100, 1)
```

Our confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote.

## Exercises

1) We have been using urn models to motivate the use of probability models. However, most data science applications are not related to data obtained from urns. More common are data that come from individuals. Probability plays a role because the data come from a random sample. The random sample is taken from a population and the urn serves as an analogy for the population.

Let's revisit the heights dataset. For now, consider x to be the heights of all males in the data set. Mathematically speaking, x is our population. Using the urn analogy, we have an urn with the values of x in it.

What are the population average and standard deviation of our population?

```{r}
# Load the 'dslabs' package and data contained in 'heights'
library(dslabs)
data(heights)

# Make a vector of heights from all males in the population
x <- heights %>% filter(sex == "Male") %>%
  .$height

# Calculate the population average. Print this value to the console.
mean(x)

# Calculate the population standard deviation. Print this value to the console.
sd(x)
```

2) Call the population average computed above μ and the standard deviation σ. Now take a sample of size 50, with replacement, and construct an estimate for μ and σ.

```{r}
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `N` as the number of people measured
N <- 50

# Define `X` as a random sample from our population `x`
X <- sample(x, N, replace = TRUE)

# Calculate the sample average. Print this value to the console.
mean(X)

# Calculate the sample standard deviation. Print this value to the console.
sd(X)
```

3) What does the central limit theory tell us about the sample average and how it is related to μ, the population average?

- It is a random sample with expected value μ and standard error σ/sqrt(N)

4) So how is this useful? We are going to use an oversimplified yet illustrative example. Suppose we want to know the average height of our male students, but we only get to measure 50 of the 708. We will use ¯X as our estimate. We know from the answer to exercise 3 that the standard estimate of our error ¯X−μ is σ/√N. We want to compute this, but we don’t know σ. Based on what is described in this section, show your estimate of σ. Now that we have an estimate of σ, let’s call our estimate s. Construct a 95% confidence interval for μ.

```{r}
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `N` as the number of people measured
N <- 50

# Define `X` as a random sample from our population `x`
X <- sample(x, N, replace = TRUE)

# Define `se` as the standard error of the estimate. Print this value to the console.
X_hat <- mean(X)
se_hat <- sd(X)
se <- se_hat / sqrt(N)
se

# Construct a 95% confidence interval for the population average based on our sample. Save the lower and then the upper confidence interval to a variable called `ci`.
alpha <- 0.05 / 2
low_interval <- qnorm(0 + alpha, X_hat, se)
up_interval <- qnorm(1 - alpha, X_hat, se)
ci <- c(low_interval, up_interval)
ci
```

5) Now run a Monte Carlo simulation in which you compute 10,000 confidence intervals as you have just done. What proportion of these intervals include μ?


```{r}
# Define `mu` as the population average
mu <- mean(x)

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `N` as the number of people measured
N <- 50

# Define `B` as the number of times to run the model
B <- 10000

# Define an object `res` that contains a logical vector for simulated intervals that contain mu
res <- replicate(B, {
  X <- sample(x, N, replace = T)
  X_hat <- mean(X)
  se_hat <- sd(X)
  se <- se_hat / sqrt(N)
  alpha <- 0.05 / 2
  low_interval <- qnorm(0 + alpha, X_hat, se)
  up_interval <- qnorm(1 - alpha, X_hat, se)
  between(mu, low_interval, up_interval)
})

mean(res)
```

6) In this section, we talked about pollster bias. We used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election. We want to answer the question: is there a poll bias? Make a plot showing the spreads for each poll.

```{r}
data(polls_us_election_2016)
polls <- polls_us_election_2016 |> 
  filter(pollster %in% c("Rasmussen Reports/Pulse Opinion Research",
                         "The Times-Picayune/Lucid") &
           enddate >= "2016-10-15" &
           state == "U.S.") |> 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

polls %>% ggplot(aes(x = pollster, y = spread))+
  geom_boxplot()+
  geom_point()

```

7) The data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance. The urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call d.

To answer the question “is there an urn model?”, we will model the observed data Yi,j in the following way:

Yi,j = d + bi + eij

i,j = 1,2 (indexes of pollster)
b = bias for pollster i
eij = poll to poll chance variability

We assumme e are independent from each other, have expected value 0 and standard deviation σi regardless of j

We must consider if bi != bj


8) What is the expected value of Yij?

d + bi


9) Expected Value and Standard Error of Poll 1

The expected value is d+b1 and standard error is σ1/sqrt(N1)

10) Expected Value and Standard Error of Poll 2

The expected value is d+b2 and standard error is σ2/sqrt(N2)

11)  Difference in Expected Values Between Polls

(d + b2) - (d + b1) = b2 - b1

12) Standard Error of the Difference Between Polls

(σ2/sqrt(N2)) - (σ1/sqrt(N1)) = sqrt(σ2/N2 + σ1/N1)

13) The answer to the question above depends on σ1 and σ2, which we don’t know. We learned that we can estimate these with the sample standard deviation. Write code that computes these two estimates.

```{r}
# Create an object called `sigma` that contains a column for `pollster` and a column for `s`, the standard deviation of the spread
sigma <- polls %>% group_by(pollster) %>% summarise(s = sd(spread))

# Print the contents of sigma to the console
sigma
```

14) What does the CLT tell us about the distribution of ¯Y2−¯Y1?

Note that ¯Y2 and ¯Y1 are sample averages, so if we assume N2 and N1 are large enough, each is approximately normal. The difference of normals is also normal.

15) Calculate the 95% Confidence Interval of the Spreads

We have constructed a random variable that has expected value b2−b1, the pollster bias difference. If our model holds, then this random variable has an approximately normal distribution and we know its standard error. The standard error depends on σ1 and σ2, but we can plug the sample standard deviations we computed above. We started off by asking: is b2−b1 different from 0? Use all the information we have learned above to construct a 95% confidence interval for the difference b2 and b1

```{r}
# Create an object called `res` that summarizes the average, standard deviation, and number of polls for the two pollsters.
res <- polls %>% group_by(pollster) %>% summarize(avg=mean(spread), s = sd(spread), N=n())

# Store the difference between the larger average and the smaller in a variable called `estimate`. Print this value to the console.
estimate <- max(res$avg) - min(res$avg)
estimate

# Store the standard error of the estimates as a variable called `se_hat`. Print this value to the console.
# sqrt(σ2/N2 + σ1/N1)
se_hat <- sqrt(res$s[2]^2/res$N[2] + res$s[1]^2/res$N[1])
se_hat

# Calculate the 95% confidence interval of the spreads. Save the lower and then the upper confidence interval to a variable called `ci`.
a <- 0.05
z <- qnorm(1-a/2)
ci <- c(estimate - z * se_hat, estimate + z * se_hat)
ci
```

16)  Calculate the P-value

```{r}
# We made an object `res` to summarize the average, standard deviation, and number of polls for the two pollsters.
res <- polls %>% group_by(pollster) %>% 
  summarize(avg = mean(spread), s = sd(spread), N = n()) 

# The variables `estimate` and `se_hat` contain the spread estimates and standard error, respectively.
estimate <- res$avg[2] - res$avg[1]
se_hat <- sqrt(res$s[2]^2/res$N[2] + res$s[1]^2/res$N[1])

# Calculate the p-value
2 * (1 - pnorm(estimate / se_hat, 0, 1))
```


## Bayesian Statistics

What does it mean when an election forecaster tells us that a given candidate has a 90% chance of winning? In the context of the urn model, this would be equivalent to stating that the probability p>0.5 is 90%. However, as we discussed earlier, in the urn model p is a fixed parameter and it does not make sense to talk about probability. With Bayesian statistics, we model p as random variable and thus a statement such as “90% chance of winning” is consistent with the approach.

Forecasters also use models to describe variability at different levels. For example, sampling variability, pollster to pollster variability, day to day variability, and election to election variability. One of the most successful approaches used for this are hierarchical models, which can be explained in the context of Bayesian statistics.

## Bayes Theorem

We start by describing Bayes theorem. We do this using a hypothetical cystic fibrosis test as an example. Suppose a test for cystic fibrosis has an accuracy of 99%. We will use the following notation

Prob (+ | D = 1) = 0.99,Prob(- | D = 0) = 0.99

+ means a positive test and D represent if you have the disease (1) or not (0)

Suppose we select a random person and they test positive. What is the probability that they have the disease? W write this as Prob (D=1 | +)?. The cystic fibrosis rate is 1 in 3,900 which implies that Prob(D=1)=0.00025. To answer this question, we will use Bayes theorem, which in general tells us that:

Pr (A | B) = (Pr(B | A) Pr(A)) / Pr(B)

In our problem

Take into account that Pr(+) = Pr(+ | D=1)\*(P(D=1)) + Pr(+ | D=0)\*Pr(D=0)

Pr (D=1 | +) = (Pr(+ | D=1) Pr(D=1)) / (Pr(+)) = (Pr(+ | D=1) Pr(D=1)) / (Pr(+ | D=1)\*(P(D=1)) + Pr(+ | D=0)\*Pr(D=0))


(0.99\*0.00025) / (0.99\*0.00025) + (0.01 * 0..99975) = 0.02

This says that despite the test having 0.99 accuracy, the probability of having the disease given a positive test is only 0.02. This may appear counter-intuitive to some, but the reason this is the case is because we have to factor in the very rare probability that a person, chosen at random, has the disease. To illustrate this, we run a Monte Carlo simulation.


## Bayes Theorem Simulation

The following simulation is meant to help you visualize Bayes theorem. We start by randomly selecting 100,000 people from a population in which the disease in question has a 1 in 4,000 prevalence.

```{r}
prev <- 1 / 4000
N <- 100000
outcome <- sample(c("Disease","Healthy"), N, replace = TRUE, 
                  prob = c(prev, 1 - prev))

N_D <- sum(outcome == "Disease")
N_D
N_H <- sum(outcome == "Healthy")
N_H
```

Also, there are many without the disease, which makes it more probable that we will see some false positives given that the test is not perfect. Now each person gets the test, which is correct 99% of the time

```{r}
accuracy <- 0.99
test <- vector("character", N)
test[outcome == "Disease"]  <- sample(c("+", "-"), N_D, replace = TRUE, 
                                    prob = c(accuracy, 1 - accuracy))
test[outcome == "Healthy"]  <- sample(c("-", "+"), N_H, replace = TRUE, 
                                    prob = c(accuracy, 1 - accuracy))
```

Because there are so many more controls than cases, even with a low false positive rate we get more controls than cases in the group that tested positive:

```{r}
table(outcome, test)
```

From this table, we see that the proportion of positive tests that have the disease is 19 out of 1015. We can run this over and over again to see that, in fact, the probability converges to about 0.02.

### Bayesian in practice

José Iglesias is a professional baseball player. In April 2013, when he was starting his career, he was performing rather well:

```{r}
data.frame(Month = "April", "At Bats" = 20, H = 9, AVG = 0.45)
```

The batting average (AVG) statistic is one way of measuring success. Roughly speaking, it tells us the success rate when batting. An AVG of .450 means José has been successful 45% of the times he has batted (At Bats) which is rather high, historically speaking. Keep in mind that no one has finished a season with an AVG of .400 or more since Ted Williams did it in 1941! To illustrate the way hierarchical models are powerful, we will try to predict José’s batting average at the end of the season. Note that in a typical season, players have about 500 at bats.

With the techniques we have learned up to now, referred to as frequentist techniques, the best we can do is provide a confidence interval. We can think of outcomes from hitting as a binomial with a success rate of p. So if the success rate is indeed .450, the standard error of just 20 at bats is:

```{r}
se <- sqrt((0.45*(1-0.450))/20)
ci <- c(0.45 - se*2, 0.45 + se*2)
ci
```

This prediction has two problems. First, it is very large, so not very useful. Second, it is centered at .450, which implies that our best guess is that this new player will break Ted Williams’ record.

If you follow baseball, this last statement will seem wrong and this is because you are implicitly using a hierarchical model that factors in information from years of following baseball. Here we show how we can quantify this intuition.

If we explore the distribution of batting averages for all players with more than 500 at bats during 3 seasons: The average player had an AVG of .275 and the standard deviation of the population of players was 0.027. So we can see already that .450 would be quite an anomaly since it is over six standard deviations away from the mean. So is José lucky or is he the best batter seen in the last 50 years? Perhaps it’s a combination of both luck and talent. But how much of each? If we become convinced that he is lucky, we should trade him to a team that trusts the .450 observation and is maybe overestimating his potential.

## Hierarchical models

The hierarchical model provides a mathematical description of how we came to see the observation of .450. First, we pick a player at random with an intrinsic ability summarized by, for example, p. Then we see 20 random outcomes with success probability p.

We use a model to represent two levels of variability in our data. First, each player is assigned a natural ability to hit. We will use the symbol p to represent this ability. You can think of p as the batting average you would converge to if this particular player batted over and over again. We assume that p has a normal distribution. With expected value .270 and standard error 0.027.

Now the second level of variability has to do with luck when batting. Regardless of how good the player is, sometimes you have bad luck and sometimes you have good luck. At each at bat, this player has a probability of success p. If we add up these successes and failures, then the CLT tells us that the observed average, call it Y, has a normal distribution with expected value p and standard error sqrt((p(1-p)/N):

We refer to the model as hierarchical because we need to know p, the first level, in order to model Y, the second level. In our example the first level describes randomness in assigning talent to a player and the second describes randomness in this particular player’s performance once we have fixed the talent parameter. In a Bayesian framework, the first level is called a prior distribution and the second the sampling distribution.

The model is:

p ~ N(u,r^2)  -- u = 0.270; r = 0.027 -- Means that the random variable follow the distribution of the right (a normal) with mean u and sd r^2
Y | p ~ N(p, sd^2) -- sd^2 = p(1-p)/N -- The | is read as conditioned on


p ~ N(0.275, 0.027^2)

Y | p ~ N(p, 111^2)

We now are ready to compute a posterior distribution to summarize our prediction of p. The continuous version of Bayes’ rule can be used here to derive the posterior probability function, which is the distribution of p assuming we observe Y=y. In our case, we can show that when we fix Y=y, p follows a normal distribution with expected value:

E(p | Y = y) = Bu + (1-B)y = u + (1-B)(y-u)

B = sd^2 / (sd^2 + r^2)

And standard error

SE(p | y²) = 1/(1/sd² + 1/r²)

This is a weighted average of the population average μ and the observed data y. The weight depends on the SD of the population τ and the SD of our observed data σ. This weighted average is sometimes referred to as shrinking because it shrinks estimates towards a prior mean. In the case of José Iglesias, we have:

E(p | Y = 0.45)

```{r}
B <- (0.111^2)/(0.111^2 + 0.027^2) # 0.944
E <- 0.275 + (1-B)*(0.45-0.275) # 0.285
E

SE <- 1/(1/.111^2 + 1/0.027^2)
SE

SD <- sqrt(SE)
```

So we started with a frequentist 95% confidence interval that ignored data from other players and summarized just José’s data: .450 ± 0.220. Then we used a Bayesian approach that incorporated data from other players and other years to obtain a posterior probability. This is actually referred to as an empirical Bayes approach because we used data to construct the prior. From the posterior, we can report what is called a 95% credible interval by reporting a region, centered at the mean, with a 95% chance of occurring. In our case, this turns out to be: .285 ± 0.052.

The Bayesian credible interval suggests that if another team is impressed by the .450 observation, we should consider trading José as we are predicting he will be just slightly above average.

## Exercises

1) In 1999, in England, Sally Clark was found guilty of the murder of two of her sons. Both infants were found dead in the morning, one in 1996 and another in 1998. In both cases, she claimed the cause of death was sudden infant death syndrome (SIDS). No evidence of physical harm was found on the two infants so the main piece of evidence against her was the testimony of Professor Sir Roy Meadow, who testified that the chances of two infants dying of SIDS was 1 in 73 million. He arrived at this figure by finding that the rate of SIDS was 1 in 8,500 and then calculating that the chance of two SIDS cases was 8,500 × 8,500 ≈ 73 million. Which of the following do you agree with?

- Sir Meadow assumed that the probability of the second son being affected by SIDS was independent of the first son being affected, thereby ignoring possible genetic causes. If genetics plays a role then: Pr(second case of SIDS∣first case of SIDS)<Pr(first case of SIDS).

2) Let’s assume that there is in fact a genetic component to SIDS and the probability of Pr(second case of SIDS∣first case of SIDS)=1/100, is much higher than 1 in 8,500. What is the probability of both of her sons dying of SIDS?

```{r}
# Define `Pr_1` as the probability of the first son dying of SIDS
Pr_1 <- 1/8500

# Define `Pr_2` as the probability of the second son dying of SIDS
Pr_2 <- 1/100

# Calculate the probability of both sons dying of SIDS. Print this value to the console.
Pr_1*Pr_2
```

3) Many press reports stated that the expert claimed the probability of Sally Clark being innocent as 1 in 73 million. Perhaps the jury and judge also interpreted the testimony this way. This probability can be written as the probability of a mother is a son-murdering psychopath given that two of her children are found dead with no evidence of physical harm. According to Bayes’ rule, what is this?

Pr(two children found deead with no evidence | mother is a murderer) Pr(mother is a murderer) / Pr(two children found deead with no evidence)

4) Assume that the chance of a son-murdering psychopath finding a way to kill her children, without leaving evidence of physical harm, is:

Pr(A | B) = 0.5

A = two of her children are found dead with no evidence of physical harm
B = a mother is a son-murdering psychopath

Assume that the rate of son-murdering psychopaths mothers is 1 in 1,000,000. According to Bayes’ theorem, what is the probability of Pr(B∣A) ?

```{r}
# Define `Pr_1` as the probability of the first son dying of SIDS
Pr_1 <- 1/8500

# Define `Pr_2` as the probability of the second son dying of SIDS
Pr_2 <- 1/100

# Define `Pr_B` as the probability of both sons dying of SIDS
Pr_B <- Pr_1*Pr_2

# Define Pr_A as the rate of mothers that are murderers
Pr_A <- 1/1000000

# Define Pr_BA as the probability that two children die without evidence of harm, given that their mother is a murderer
Pr_BA <- 0.50

# Define Pr_AB as the probability that a mother is a murderer, given that her two children died with no evidence of physical harm. Print this value to the console.
Pr_AB <- Pr_BA*Pr_A/Pr_B
Pr_AB
```

5) After Sally Clark was found guilty, the Royal Statistical Society issued a statement saying that there was “no statistical basis” for the expert’s claim. They expressed concern at the “misuse of statistics in the courts”. Eventually, Sally Clark was acquitted in June 2003. What did the expert miss?

- He did not take into account how rare it is for a mother to murder her children

6) Florida is one of the most closely watched states in the U.S. election because it has many electoral votes, and the election is generally close, and Florida tends to be a swing state that can vote either way. Create the following table with the polls taken during the last two weeks:

```{r}
library(tidyverse)
library(dslabs)
data(polls_us_election_2016)
polls <- polls_us_election_2016 |> 
  filter(state == "Florida" & enddate >= "2016-11-04" ) |> 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
```

Take the average spread of these polls. The CLT tells us this average is approximately normal. Calculate an average and provide an estimate of the standard error. Save your results in an object called results.

```{r}
results <- polls %>% summarize(avg = mean(spread),  se = sd(spread)/sqrt(n()))
```

7) Now assume a Bayesian model that sets the prior distribution for Florida’s election night spread d to be Normal with expected value μ and standard deviation τ. What are the interpretations of μ and τ?

-  μ and τ summarize what we would predict for Florida before seeing any polls.

8) The CLT tells us that our estimate of the spread ^d has normal distribution with expected value d and standard deviation σ calculated in problem 6. Use the formulas we showed for the posterior distribution to calculate the expected value of the posterior distribution if we set μ=0 and τ=0.01. Then, calculate standard deviation:

- Define μ and τ Identify which elements stored in the object results represent σ and Y
- Estimate B using σ and τ
- Estimate the posterior distribution using B, μ, and Y


```{r}
# Define `mu` and `tau`
mu <- 0
tau <- 0.01

# Define a variable called `sigma` that contains the standard error in the object `results
sigma <- results$se

# Define a variable called `Y` that contains the average in the object `results`
Y <- results$avg

# Define a variable `B` using `sigma` and `tau`. Print this value to the console.
#B = sd^2 / (sd^2 + r^2)
B <- sigma^2 / (sigma^2 + tau^2)

# Calculate the expected value of the posterior distribution
#E(p | Y = y) = Bu + (1-B)y = u + (1-B)(y-u)
mu + (1-B)*(Y-mu)

# Compute the standard error of the posterior distribution. Print this value to the console.
sqrt(1 / (1 / sigma ^2 + 1 / tau ^2))
```

9) Using the fact that the posterior distribution is normal, create an interval that has a 95% probability of occurring centered at the posterior expected value. Note that we call these credible intervals.

```{r}
# Here are the variables we have defined in previous exercises
mu <- 0
tau <- 0.01
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)
se <- sqrt( 1/ (1/sigma^2 + 1/tau^2))

# Construct the 95% credible interval. Save the lower and then the upper confidence interval to a variable called `ci`.
est <- B * mu + (1 - B) * Y
est

ci <- c(est - qnorm(0.975) * se, est + qnorm(0.975) * se)
ci
```






# Association Tests

The statistical tests we have studied up to now leave out a substantial portion of data types. Specifically, we have not discussed inference for binary, categorical, and ordinal data. To give a very specific example, consider the following case study.

A 2014 PNAS paper analyzed success rates from funding agencies in the Netherlands and concluded that their:

"results reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language use in instructional and evaluation materials"

```{r}
library(tidyverse)
library(dslabs)
data("research_funding_rates")
research_funding_rates |> select(discipline, applications_total, 
                                  success_rates_total) |> head()


totals <- research_funding_rates |> 
  select(-discipline) |> 
  summarize_all(sum) |>
  summarize(yes_men = awards_men, 
            no_men = applications_men - awards_men, 
            yes_women = awards_women, 
            no_women = applications_women - awards_women) 

totals |> summarize(percent_men = yes_men/(yes_men+no_men),
                     percent_women = yes_women/(yes_women+no_women))
```

So we see that a larger percent of men than women received awards. But could this be due just to random variability? Here we learn how to perform inference for this type of data.

## Lady Tasting Tea

R.A. Fisher was one of the first to formalize hypothesis testing. The “Lady Tasting Tea” is one of the most famous examples.

The story is as follows: an acquaintance of Fisher’s claimed that she could tell if milk was added before or after tea was poured. Fisher was skeptical. He designed an experiment to test this claim. He gave her four pairs of cups of tea: one with milk poured first, the other after. The order was randomized. The null hypothesis here is that she is guessing. Fisher derived the distribution for the number of correct picks on the assumption that the choices were random and independent.

As an example, suppose she picked 3 out of 4 correctly. Do we believe she has a special ability? The basic question we ask is: if the tester is actually guessing, what are the chances that she gets 3 or more correct? Just as we have done before, we can compute a probability under the null hypothesis that she is guessing 4 of each. Under this null hypothesis, we can think of this particular example as picking 4 balls out of an urn with 4 blue (correct answer) and 4 red (incorrect answer) balls. Remember, she knows that there are four before tea and four after.

Under the null hypothesis that she is simply guessing, each ball has the same chance of being picked. We can then use combinations to figure out each probability. 

The probability of picking 3 is (4/3)(4/1)/(8/4) = 16/70

The probability of picking 4 is (4/4)(4/0)/(8/4) = 1/70

Thus, the chance of observing a 3 or something more extreme, under the null hypothesis, is ≈0.24. This is the p-value. The procedure that produced this p-value is called Fisher’s exact test and it uses the hypergeometric distribution.

## Two-by-Two Tables

The data from the experiment is usually summarized by a table like this:

```{r}
tab <- matrix(c(3,1,1,3),2,2)
rownames(tab)<-c("Poured Before","Poured After")
colnames(tab)<-c("Guessed before","Guessed after")
tab

fisher.test(tab, alternative="greater")$p.value
```

## Chi-Square Test

Notice that, in a way, our funding rates example is similar to the Lady Tasting Tea. However, in the Lady Tasting Tea example, the number of blue and red beads is experimentally fixed and the number of answers given for each category is also fixed. This is because Fisher made sure there were four cups with milk poured before tea and four cups with milk poured after and the lady knew this, so the answers would also have to include four befores and four afters. If this is the case, the sum of the rows and the sum of the columns are fixed. This defines constraints on the possible ways we can fill the two by two table and also permits us to use the hypergeometric distribution. In general, this is not the case. Nonetheless, there is another approach, the Chi-squared test, which is described below.

Imagine we have 290, 1,345, 177, 1,011 applicants, some are men and some are women and some get funded, whereas others don’t. We saw that the success rates for men and woman were:

```{r}
totals |> summarize(percent_men = yes_men/(yes_men+no_men),
                     percent_women = yes_women/(yes_women+no_women))
```

respectively. Would we see this again if we randomly assign funding at the overall rate:

```{r}
rate <- totals |>
  summarize(percent_total = 
              (yes_men + yes_women)/
              (yes_men + no_men +yes_women + no_women)) |>
  pull(percent_total)
rate
```

The Chi-square test answers this question. The first step is to create the two-by-two data table:

```{r}
two_by_two <- data.frame(awarded = c("no", "yes"), 
                     men = c(totals$no_men, totals$yes_men),
                     women = c(totals$no_women, totals$yes_women))
two_by_two
```

The general idea of the Chi-square test is to compare this two-by-two table to what you expect to see

```{r}
data.frame(awarded = c("no", "yes"), 
       men = (totals$no_men + totals$yes_men) * c(1 - rate, rate),
       women = (totals$no_women + totals$yes_women) * c(1 - rate, rate))
```

We can see that more men than expected and fewer women than expected received funding. However, under the null hypothesis these observations are random variables. The Chi-square test tells us how likely it is to see a deviation this large or larger. This test uses an asymptotic result, similar to the CLT, related to the sums of independent binary outcomes. The R function chisq.test takes a two-by-two table and returns the results from the test:

```{r}
chisq_test <- two_by_two |> select(-awarded) |> chisq.test()
chisq_test$p.value
```

## Odds Ratio

An informative summary statistic associated with two-by-two tables is the odds ratio. Define the two variables as X=1 if you are a male and 0 otherwise, and Y=1 if you are funded and 0 otherwise. The odds of getting funded if you are a man is defined:

Pr(Y=1∣X=1)/Pr(Y=0∣X=1)

```{r}
odds_men <- with(two_by_two, (men[2]/sum(men)) / (men[1]/sum(men)))
odds_men
```

And the odds of being funded if you are a woman is:

Pr(Y=1∣X=0)/Pr(Y=0∣X=0)

```{r}
odds_women <- with(two_by_two, (women[2]/sum(women)) / (women[1]/sum(women)))
odds_women
```

The odds ratio is the ratio for these two odds: how many times larger are the odds for men than for women?

```{r}
odds_men / odds_women
```

```{r}
data.frame(awarded = c("yes", "no"), 
       men = c("a","c"),
       women = c("b","d"))
```

odds ratio = (ad)/(bc)


## Confidence intervals for the odds ratio

Computing confidence intervals for the odds ratio is not mathematically straightforward. Unlike other statistics, for which we can derive useful approximations of their distributions, the odds ratio is not only a ratio, but a ratio of ratios. Therefore, there is no simple way of using, for example, the CLT.

However, statistical theory tells us that when all four entries of the two-by-two table are large enough, then the log of the odds ratio is approximately normal with standard error

sqrt(1/a+1/b+1/c+1/d)

This implies that a 95% confidence interval for the log odds ratio can be formed by:

log(ad/bc) +- 1.96*sqrt(1/a+1/b+1/c+1/d)

```{r}
log_or <- log(odds_men / odds_women)
se <- two_by_two |> select(-awarded) |>
  summarize(se = sqrt(sum(1/men) + sum(1/women))) |>
  pull(se)
ci <- log_or + c(-1,1) * qnorm(0.975) * se
odds_ratio <- exp(ci)
odds_ratio
```

Note that 1 is not included in the confidence interval which must mean that the p-value is smaller than 0.05. We can confirm this using:

```{r}
2*(1 - pnorm(log_or, 0, se))
```

This is a slightly different p-value than that with the Chi-square test. This is because we are using a different asymptotic approximation to the null distribution. To learn more about inference and asymptotic theory for odds ratio, consult the Generalized Linear Models book by McCullagh and Nelder.

## Large samples, small p-values

As mentioned earlier, reporting only p-values is not an appropriate way to report the results of data analysis. In scientific journals, for example, some studies seem to overemphasize p-values. Some of these studies have large sample sizes and report impressively small p-values. Yet when one looks closely at the results, we realize odds ratios are quite modest: barely bigger than 1. In this case the difference may not be practically significant or scientifically significant.

Note that the relationship between odds ratio and p-value is not one-to-one. It depends on the sample size. So a very small p-value does not necessarily mean a very large odds ratio. Notice what happens to the p-value if we multiply our two-by-two table by 10, which does not change the odds ratio:

```{r}
two_by_two_x_10 <- two_by_two |> 
  select(-awarded) |>
  mutate(men = men*10, women = women*10) 
chisq.test(two_by_two_x_10)$p.value
```


# Exercises

1) A famous athlete has an impressive career, winning 70% of her 500 career matches. However, this athlete gets criticized because in important events, such as the Olympics, she has a losing record of 8 wins and 9 losses. Perform a Chi-square test to determine if this losing record can be simply due to chance as opposed to not performing well under pressure.

```{r}
N <- 500
win_rate <- 0.7
wins <- N * win_rate
losses <- 500 - wins

athlete_performance <- matrix(c(wins, losses, 8, 9),2,2)
rownames(athlete_performance) <- c("wins", "losses")
colnames(athlete_performance) <- c("career","olympics")
athlete_performance

chisq_test <- athlete_performance |> chisq.test()
chisq_test$p.value

# As p > 0.05 we do not reject H0 =>  a p-value of 0.08 suggests that there may not be strong evidence to conclude that the athlete's performance in career matches and in the Olympics are significantly related. we cannot confirm that the losing record in important events is simply due to chance.
```

2) Why did we use the Chi-square test instead of Fisher’s exact test in the previous exercise? - 


```{r}
fisher_test <- athlete_performance |> fisher.test()
fisher_test$p.value
```

3) Compute odds ratio of "losing under pressure" along with a confidence interval

```{r}
athlete_performance <- as.data.frame(athlete_performance)
odds_career <- with(athlete_performance, (career[2]/sum(career)) / (career[1]/sum(career)))
odds_olympics <- with(athlete_performance, (olympics[2]/sum(olympics)) / (olympics[1]/sum(olympics)))
odds_ratio <- odds_career / odds_olympics

log_or <- log(odds_career / odds_olympics)
se <- athlete_performance |>
  summarize(se = sqrt(sum(1/career) + sum(1/olympics))) |>
  pull(se)
ci <- log_or + c(-1,1) * qnorm(0.975) * se
odds_ratio <- exp(ci)
odds_ratio
```

